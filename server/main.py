from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import json
import os
from datetime import datetime
import chromadb
from chromadb.utils import embedding_functions

app = FastAPI(title="Gemini Memory Bridge (Vector RAG Edition)")

# --- é…ç½® ---
DATA_FILE = "memory.json"
CHROMA_PATH = "chroma_db"

# åˆå§‹åŒ– ChromaDB
# ä½¿ç”¨æŒä¹…åŒ–å®¢æˆ·ç«¯
client = chromadb.PersistentClient(path=CHROMA_PATH)

# ä½¿ç”¨æ”¯æŒä¸­æ–‡çš„å¤šè¯­è¨€æ¨¡å‹
# ç¬¬ä¸€æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ (çº¦ 470MB)
sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="paraphrase-multilingual-MiniLM-L12-v2")

collection = client.get_or_create_collection(
    name="gemini_memory", 
    embedding_function=sentence_transformer_ef
)

class MemoryItem(BaseModel):
    content: str
    timestamp: Optional[str] = None
    tags: List[str] = []

class QueryRequest(BaseModel):
    user_input: str

class MemoryResponse(BaseModel):
    context: str
    source_count: int

# --- è¾…åŠ©å‡½æ•° ---
def load_json_memories() -> List[dict]:
    if not os.path.exists(DATA_FILE):
        return []
    with open(DATA_FILE, "r", encoding="utf-8") as f:
        try:
            return json.load(f)
        except json.JSONDecodeError:
            return []

def save_json_memory(content: str, tags: list = None):
    memories = load_json_memories()
    timestamp = datetime.now().isoformat()
    new_memory = {
        "content": content,
        "timestamp": timestamp,
        "tags": tags or []
    }
    memories.append(new_memory)
    with open(DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(memories, f, ensure_ascii=False, indent=2)
    return new_memory, timestamp

# --- è¿ç§»é€»è¾‘ (Migration) ---
# æ¯æ¬¡å¯åŠ¨æ—¶æ£€æŸ¥ï¼Œå¦‚æœ Chroma æ˜¯ç©ºçš„ä½† JSON æœ‰æ•°æ®ï¼Œå°±å¯¼è¿›å»
def migrate_json_to_chroma():
    existing_count = collection.count()
    if existing_count == 0:
        json_data = load_json_memories()
        if json_data:
            print(f"ğŸ”„ Migrating {len(json_data)} memories from JSON to Vector DB...")
            ids = [f"mem_{i}" for i in range(len(json_data))]
            documents = [m["content"] for m in json_data]
            metadatas = [{"timestamp": m["timestamp"], "tags": ",".join(m["tags"])} for m in json_data]
            
            collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
            print("âœ… Migration complete.")

# æ‰§è¡Œè¿ç§»
migrate_json_to_chroma()

# --- æ ¸å¿ƒ API ---

@app.get("/")
def read_root():
    return {"status": "running", "mode": "Vector RAG", "count": collection.count()}

@app.post("/add_memory")
def add_memory(item: MemoryItem):
    """
    ä¿å­˜è®°å¿†ï¼šåŒæ—¶å†™å…¥ JSON (å¤‡ä»½) å’Œ ChromaDB (æ£€ç´¢)
    """
    # 1. å­˜ JSON
    saved_item, timestamp = save_json_memory(item.content, item.tags)
    
    # 2. å­˜ ChromaDB
    # ç”Ÿæˆå”¯ä¸€ ID (ç®€å•èµ·è§ç”¨æ—¶é—´æˆ³+å“ˆå¸Œï¼Œæˆ–è€… UUID)
    import hashlib
    doc_id = hashlib.md5((item.content + timestamp).encode()).hexdigest()
    
    collection.add(
        documents=[item.content],
        metadatas=[{"timestamp": timestamp, "tags": ",".join(item.tags)}],
        ids=[doc_id]
    )
    
    print(f"ğŸ“¥ Saved memory: {item.content[:30]}...")
    return {"status": "success", "data": saved_item}

@app.post("/search_context", response_model=MemoryResponse)
def search_context(query: QueryRequest):
    """
    å‘é‡æ£€ç´¢æ¥å£
    """
    user_text = query.user_input
    
    # æ‰§è¡ŒæŸ¥è¯¢
    results = collection.query(
        query_texts=[user_text],
        n_results=3  # è¿”å›æœ€ç›¸å…³çš„ 3 æ¡
    )
    
    # è§£æç»“æœ
    # results['documents'] æ˜¯ä¸€ä¸ª list of list
    if not results['documents'] or not results['documents'][0]:
        return {"context": "", "source_count": 0}

    # ç®€å•çš„è·ç¦»é˜ˆå€¼è¿‡æ»¤ (å¯é€‰)
    # results['distances'] è¶Šå°è¶Šç›¸ä¼¼ (L2 è·ç¦»)
    # å¯¹äº paraphrase-multilingual-MiniLM-L12-v2, è·ç¦»é€šå¸¸åœ¨ 0 ~ 2 ä¹‹é—´
    # ç»éªŒé˜ˆå€¼: < 1.2 è¡¨ç¤ºæœ‰ä¸€å®šç›¸å…³æ€§, < 0.8 è¡¨ç¤ºå¼ºç›¸å…³
    THRESHOLD = 1.2
    
    found_docs = results['documents'][0]
    found_distances = results['distances'][0]
    
    valid_docs = []
    for doc, dist in zip(found_docs, found_distances):
        print(f"ğŸ” Match: {doc[:20]}... (Dist: {dist:.4f})")
        
        if dist < THRESHOLD:
            valid_docs.append(doc)

    if not valid_docs:
        print("   -> No documents met the threshold.")
        return {"context": "", "source_count": 0}

    context_text = "ã€æœ¬åœ°è®°å¿†åº“æç¤ºã€‘:\n" + "\n".join([f"- {d}" for d in valid_docs])
    
    return {"context": context_text, "source_count": len(valid_docs)}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)