from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
from datetime import datetime
import chromadb
from chromadb.utils import embedding_functions

app = FastAPI(title="Gemini Memory Bridge (Vector RAG Only)")

# --- é…ç½® ---
CHROMA_PATH = "chroma_db"

# åˆå§‹åŒ– ChromaDB
# ä½¿ç”¨æŒä¹…åŒ–å®¢æˆ·ç«¯
client = chromadb.PersistentClient(path=CHROMA_PATH)

# ä½¿ç”¨æ”¯æŒä¸­æ–‡çš„å¤šè¯­è¨€æ¨¡å‹
# ç¬¬ä¸€æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ (çº¦ 470MB)
sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="paraphrase-multilingual-MiniLM-L12-v2")

collection = client.get_or_create_collection(
    name="gemini_memory", 
    embedding_function=sentence_transformer_ef
)

class MemoryItem(BaseModel):
    content: str
    timestamp: Optional[str] = None
    tags: List[str] = []

class QueryRequest(BaseModel):
    user_input: str

class MemoryResponse(BaseModel):
    context: str
    source_count: int

# --- æ ¸å¿ƒ API ---

@app.get("/")
def read_root():
    return {"status": "running", "mode": "Vector RAG Only", "count": collection.count()}

@app.post("/add_memory")
def add_memory(item: MemoryItem):
    """
    ä¿å­˜è®°å¿†ï¼šåªå†™å…¥ ChromaDB (æ£€ç´¢)
    """
    timestamp = datetime.now().isoformat()
    tags = item.tags or []
    
    # ç”Ÿæˆå”¯ä¸€ ID (ç®€å•èµ·è§ç”¨æ—¶é—´æˆ³+å“ˆå¸Œï¼Œæˆ–è€… UUID)
    import hashlib
    doc_id = hashlib.md5((item.content + timestamp).encode()).hexdigest()
    
    collection.add(
        documents=[item.content],
        metadatas=[{"timestamp": timestamp, "tags": ",".join(tags)}],
        ids=[doc_id]
    )
    
    print(f"ğŸ“¥ Saved memory: {item.content[:30]}...")
    return {
        "status": "success", 
        "data": {
            "content": item.content,
            "timestamp": timestamp,
            "tags": tags
        }
    }

@app.post("/search_context", response_model=MemoryResponse)
def search_context(query: QueryRequest):
    """
    å‘é‡æ£€ç´¢æ¥å£
    """
    user_text = query.user_input
    
    # æ‰§è¡ŒæŸ¥è¯¢
    results = collection.query(
        query_texts=[user_text],
        n_results=3  # è¿”å›æœ€ç›¸å…³çš„ 3 æ¡
    )
    
    # è§£æç»“æœ
    # results['documents'] æ˜¯ä¸€ä¸ª list of list
    if not results['documents'] or not results['documents'][0]:
        return {"context": "", "source_count": 0}

    # ç®€å•çš„è·ç¦»é˜ˆå€¼è¿‡æ»¤ (å¯é€‰)
    # results['distances'] è¶Šå°è¶Šç›¸ä¼¼ (L2 è·ç¦»)
    # å¯¹äº paraphrase-multilingual-MiniLM-L12-v2, è·ç¦»é€šå¸¸åœ¨ 0 ~ 2 ä¹‹é—´
    # ç»éªŒé˜ˆå€¼: < 1.2 è¡¨ç¤ºæœ‰ä¸€å®šç›¸å…³æ€§, < 0.8 è¡¨ç¤ºå¼ºç›¸å…³
    THRESHOLD = 1.2
    
    found_docs = results['documents'][0]
    found_distances = results['distances'][0]
    
    valid_docs = []
    for doc, dist in zip(found_docs, found_distances):
        print(f"ğŸ” Match: {doc[:20]}... (Dist: {dist:.4f})")
        
        if dist < THRESHOLD:
            valid_docs.append(doc)

    if not valid_docs:
        print("   -> No documents met the threshold.")
        return {"context": "", "source_count": 0}

    context_text = "ã€æœ¬åœ°è®°å¿†åº“æç¤ºã€‘:\n" + "\n".join([f"- {d}" for d in valid_docs])
    
    return {"context": context_text, "source_count": len(valid_docs)}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
