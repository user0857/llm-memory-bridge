import os
import sys
import requests
import google.generativeai as genai
from datetime import datetime

# --- é…ç½® ---
BRIDGE_SERVER_URL = "http://127.0.0.1:8000"
API_KEY = os.getenv("GEMINI_API_KEY") # è¯·ç¡®ä¿ç¯å¢ƒå˜é‡é‡Œæœ‰è¿™ä¸ª

if not API_KEY:
    print("âŒ é”™è¯¯: æœªæ‰¾åˆ° GEMINI_API_KEY ç¯å¢ƒå˜é‡ã€‚")
    print("è¯·æ‰§è¡Œ: export GEMINI_API_KEY='ä½ çš„key'")
    sys.exit(1)

genai.configure(api_key=API_KEY)

# --- Bridge API ---
def get_context_from_bridge(query):
    try:
        resp = requests.post(f"{BRIDGE_SERVER_URL}/search_context", json={"user_input": query}, timeout=2)
        if resp.status_code == 200:
            data = resp.json()
            return data.get("context", "")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¿æ¥è®°å¿†æœåŠ¡å™¨: {e}")
    return ""

def save_memory_to_bridge(content):
    try:
        requests.post(
            f"{BRIDGE_SERVER_URL}/add_memory", 
            json={"content": content, "tags": ["cli-chat"]},
            timeout=2
        )
    except Exception:
        pass # é™é»˜å¤±è´¥ï¼Œä¸æ‰“æ–­å¯¹è¯

# --- ä¸»ç¨‹åº ---
def main():
    model = genai.GenerativeModel('gemini-pro')
    chat = model.start_chat(history=[])
    
    print("\nğŸ¤– Gemini CLI (Memory Synced)")
    print("--------------------------------")
    print("æç¤º: è¾“å…¥ 'exit' é€€å‡ºã€‚")

    while True:
        try:
            user_input = input("\nYou: ").strip()
            if not user_input: continue
            if user_input.lower() in ['exit', 'quit']: break

            # 1. RAG: å»æœ¬åœ°æœåŠ¡å™¨æŸ¥è®°å¿†
            context_prompt = ""
            retrieved_context = get_context_from_bridge(user_input)
            
            if retrieved_context:
                print(f"   (ğŸ”— å·²å…³è”æœ¬åœ°è®°å¿†)")
                # æ„é€ åŒ…å«è®°å¿†çš„ Prompt
                # æ³¨æ„ï¼šåœ¨ Chat æ¨¡å¼ä¸‹ï¼Œé€šå¸¸å»ºè®®æŠŠ Context æ”¾åœ¨ System Instruction é‡Œï¼Œ
                # ä½†è¿™é‡Œä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬ç›´æ¥é™„ç€åœ¨ User Message é‡Œ
                context_prompt = f"{retrieved_context}\n\nåŸºäºä»¥ä¸ŠèƒŒæ™¯ï¼Œè¯·å›ç­”ï¼š\n"

            # 2. å‘é€ç»™ Gemini
            full_prompt = context_prompt + user_input
            response = chat.send_message(full_prompt, stream=True)
            
            print("Gemini: ", end="", flush=True)
            full_response_text = ""
            for chunk in response:
                text = chunk.text
                print(text, end="", flush=True)
                full_response_text += text
            print("\n")

            # 3. åŒå‘åŒæ­¥ï¼šæŠŠè¿™æ¬¡å¯¹è¯å­˜å› Server
            # ä¿å­˜ç”¨æˆ·çš„è¯
            save_memory_to_bridge(f"CLI User: {user_input}")
            # ä¿å­˜ AI çš„è¯
            save_memory_to_bridge(f"CLI Gemini: {full_response_text}")

        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"\nâŒ Error: {e}")

if __name__ == "__main__":
    main()
