import os
import sys
import json
import requests
import google.generativeai as genai
from google.generativeai.types import content_types
from collections.abc import Iterable
from pathlib import Path

# --- é…ç½® ---
BRIDGE_SERVER_URL = "http://127.0.0.1:8000"
API_KEY = os.getenv("GEMINI_API_KEY")
HISTORY_FILE = Path("chat_history.json")

if not API_KEY:
    print("âŒ é”™è¯¯: æœªæ‰¾åˆ° GEMINI_API_KEY ç¯å¢ƒå˜é‡ã€‚")
    sys.exit(1)

genai.configure(api_key=API_KEY)

# --- Tool Functions (ä¾› Gemini è°ƒç”¨) ---
def search_memory_tool(query: str):
    """
    Search the long-term memory for relevant context.
    Use this when the user asks about past events, preferences, or specific project details.
    """
    print(f"  ğŸ” [Tool] Searching memory for: '{query}'...")
    try:
        resp = requests.post(f"{BRIDGE_SERVER_URL}/search_context", json={"user_input": query}, timeout=5)
        if resp.status_code == 200:
            data = resp.json()
            ctx = data.get("context", "")
            if ctx:
                return ctx
            return "No relevant memories found."
    except Exception as e:
        return f"Error connecting to memory bridge: {e}"
    return "No result."

def save_memory_tool(content: str, tags: str = ""):
    """
    Save important information to long-term memory.
    Use this when the user explicitly asks to remember something, or shares significant personal/project info.
    Args:
        content: The text to remember.
        tags: Comma-separated tags (e.g. "project,preference").
    """
    print(f"  ğŸ’¾ [Tool] Saving memory: '{content}'...")
    tag_list = [t.strip() for t in tags.split(",") if t.strip()]
    try:
        requests.post(
            f"{BRIDGE_SERVER_URL}/add_memory", 
            json={"content": content, "tags": tag_list},
            timeout=5
        )
        return "Memory saved successfully."
    except Exception as e:
        return f"Error saving memory: {e}"

# å·¥å…·æ˜ å°„è¡¨
tools_map = {
    'search_memory_tool': search_memory_tool,
    'save_memory_tool': save_memory_tool
}

# --- Session Management ---
# ç®€åŒ–ç‰ˆå†å²è®°å½•ï¼Œä¸»è¦ç”¨äºæ¢å¤ï¼Œä½†åœ¨ Function Calling åœºæ™¯ä¸‹
# å¤æ‚çš„ FunctionResponse åºåˆ—åŒ–æ¯”è¾ƒéº»çƒ¦ï¼Œè¿™é‡Œæš‚æ—¶åªä¿å­˜ç®€å•çš„æ–‡æœ¬äº¤äº’ä½œä¸ºä¸Šä¸‹æ–‡æ¢å¤å‚è€ƒ
# æˆ–è€…å®Œå…¨é‡ç½®ä»¥ä¿è¯å·¥å…·è°ƒç”¨çš„è¿è´¯æ€§ã€‚
def load_chat_history():
    # æš‚æ—¶ç¦ç”¨å†å²æ¢å¤ï¼Œå› ä¸º Tool Call çš„å†å²ç»“æ„æ¯”è¾ƒå¤æ‚ï¼Œ
    # ç®€å•çš„ JSON æ¢å¤å®¹æ˜“å¯¼è‡´ SDK æŠ¥é”™ã€‚
    # å»ºè®®æ¯æ¬¡å¯åŠ¨éƒ½æ˜¯æ–°ä¼šè¯ï¼Œä½†æ‹¥æœ‰é•¿æœŸè®°å¿†åº“ã€‚
    return []

# --- ä¸»ç¨‹åº ---
def main():
    # 1. åˆå§‹åŒ–æ¨¡å‹ï¼Œç»‘å®šå·¥å…·
    tools = [search_memory_tool, save_memory_tool]
    model = genai.GenerativeModel('gemini-1.5-flash', tools=tools) # ä½¿ç”¨æ”¯æŒå·¥å…·æ›´å¥½çš„æ¨¡å‹
    
    # å¼€å¯è‡ªåŠ¨å‡½æ•°è°ƒç”¨ (Auto-function calling)
    # SDK ä¼šè‡ªåŠ¨å¤„ç† function_call -> function_response çš„å¾€è¿”
    chat = model.start_chat(enable_automatic_function_calling=True)
    
    print("\nğŸ¤– Gemini CLI (Tool Use / Agent Mode)")
    print("-------------------------------------")
    print("æç¤º: æˆ‘ç°åœ¨æœ‰è‡ªä¸»æƒï¼Œä¼šæ ¹æ®éœ€è¦æŸ¥é˜…è®°å¿†æˆ–è®°å½•ä¿¡æ¯ã€‚")
    print("      è¾“å…¥ '/recall <query>' å¯å¼ºåˆ¶æ‰‹åŠ¨æ£€ç´¢ã€‚")

    while True:
        try:
            user_input = input("\nYou: ").strip()
            if not user_input: continue
            
            if user_input.lower() in ['exit', 'quit']: 
                break
            
            # æ‰‹åŠ¨æŒ‡ä»¤ä¿ç•™
            if user_input.lower().startswith('/recall'):
                q = user_input[7:].strip()
                print(search_memory_tool(q))
                continue

            # å‘é€ç»™ Gemini (SDK è‡ªåŠ¨å¤„ç†å·¥å…·è°ƒç”¨)
            response = chat.send_message(user_input)
            
            # æ‰“å°å›å¤
            print(f"Gemini: {response.text}")

        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"\nâŒ Error: {e}")

if __name__ == "__main__":
    main()
