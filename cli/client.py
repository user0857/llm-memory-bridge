import os
import sys
import json
import requests
import google.generativeai as genai
from datetime import datetime
from pathlib import Path

# --- é…ç½® ---
BRIDGE_SERVER_URL = "http://127.0.0.1:8000"
API_KEY = os.getenv("GEMINI_API_KEY") # è¯·ç¡®ä¿ç¯å¢ƒå˜é‡é‡Œæœ‰è¿™ä¸ª
HISTORY_FILE = Path("chat_history.json")

if not API_KEY:
    print("âŒ é”™è¯¯: æœªæ‰¾åˆ° GEMINI_API_KEY ç¯å¢ƒå˜é‡ã€‚")
    print("è¯·æ‰§è¡Œ: export GEMINI_API_KEY='ä½ çš„key'")
    sys.exit(1)

genai.configure(api_key=API_KEY)

# --- Bridge API ---
def get_context_from_bridge(query):
    try:
        resp = requests.post(f"{BRIDGE_SERVER_URL}/search_context", json={"user_input": query}, timeout=2)
        if resp.status_code == 200:
            data = resp.json()
            return data.get("context", "")
    except Exception as e:
        # print(f"âš ï¸ æ— æ³•è¿æ¥è®°å¿†æœåŠ¡å™¨: {e}")
        pass
    return ""

def save_memory_to_bridge(content):
    try:
        requests.post(
            f"{BRIDGE_SERVER_URL}/add_memory", 
            json={"content": content, "tags": ["cli-chat"]},
            timeout=2
        )
    except Exception:
        pass # é™é»˜å¤±è´¥ï¼Œä¸æ‰“æ–­å¯¹è¯

# --- Session Management ---
def load_chat_history():
    if not HISTORY_FILE.exists():
        return []
    
    try:
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            # è½¬æ¢ä¸º genai éœ€è¦çš„æ ¼å¼
            history = []
            for item in data:
                history.append({
                    "role": item["role"],
                    "parts": item["parts"]
                })
            return history
    except Exception as e:
        print(f"âš ï¸ æ— æ³•åŠ è½½å†å²è®°å½•: {e}")
        return []

def save_chat_history(history):
    data = []
    for entry in history:
        # entry æ˜¯ google.ai.generativelanguage.Content ç±»å‹
        parts = []
        for part in entry.parts:
            parts.append(part.text)
        
        data.append({
            "role": entry.role,
            "parts": parts
        })
        
    try:
        with open(HISTORY_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ æ— æ³•ä¿å­˜å†å²è®°å½•: {e}")

# --- ä¸»ç¨‹åº ---
def main():
    history = load_chat_history()
    model = genai.GenerativeModel('gemini-pro')
    chat = model.start_chat(history=history)
    
    print("\nğŸ¤– Gemini CLI (Memory Synced + Session Restore)")
    print("--------------------------------")
    if history:
        print(f"ğŸ”„ å·²æ¢å¤ä¹‹å‰çš„å¯¹è¯ ({len(history)} æ¡æ¶ˆæ¯)")
    print("æç¤º: è¾“å…¥ 'exit' é€€å‡ºï¼Œè¾“å…¥ '/clear' æ¸…é™¤å½“å‰ä¼šè¯ã€‚")

    while True:
        try:
            user_input = input("\nYou: ").strip()
            if not user_input: continue
            
            if user_input.lower() in ['exit', 'quit']: 
                break
            
            if user_input.lower() == '/clear':
                if HISTORY_FILE.exists():
                    os.remove(HISTORY_FILE)
                chat = model.start_chat(history=[])
                print("ğŸ§¹ ä¼šè¯å·²é‡ç½®ã€‚")
                continue

            if user_input.lower().startswith('/recall'):
                query = user_input[7:].strip()
                if not query:
                    print("âš ï¸ è¯·è¾“å…¥æŸ¥è¯¢å†…å®¹ï¼Œä¾‹å¦‚: /recall èƒèŸ¹")
                    continue
                
                print(f"ğŸ” æ­£åœ¨æ£€ç´¢å…³äº '{query}' çš„è®°å¿†...")
                ctx = get_context_from_bridge(query)
                if ctx:
                    print(f"âœ… æ£€ç´¢ç»“æœ:\n{ctx}")
                else:
                    print("ğŸ“­ æœªæ‰¾åˆ°ç›¸å…³è®°å¿† (å¯èƒ½æ˜¯ç›¸ä¼¼åº¦è¿‡ä½æˆ–æ— æ•°æ®)")
                continue

            # 1. RAG: å»æœ¬åœ°æœåŠ¡å™¨æŸ¥è®°å¿†
            context_prompt = ""
            retrieved_context = get_context_from_bridge(user_input)
            
            if retrieved_context:
                print(f"   (ğŸ”— å·²å…³è”æœ¬åœ°è®°å¿†)")
                context_prompt = f"{retrieved_context}\n\nåŸºäºä»¥ä¸ŠèƒŒæ™¯ï¼Œè¯·å›ç­”ï¼š\n"

            # 2. å‘é€ç»™ Gemini
            full_prompt = context_prompt + user_input
            
            # æ•è·å¯èƒ½çš„ API é”™è¯¯
            try:
                response = chat.send_message(full_prompt, stream=True)
                
                print("Gemini: ", end="", flush=True)
                full_response_text = ""
                for chunk in response:
                    text = chunk.text
                    print(text, end="", flush=True)
                    full_response_text += text
                print("\n")

                # 3. åŒå‘åŒæ­¥
                save_chat_history(chat.history)
                save_memory_to_bridge(f"CLI User: {user_input}")
                save_memory_to_bridge(f"CLI Gemini: {full_response_text}")
                
            except Exception as api_err:
                print(f"\nâŒ API Error: {api_err}")

        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"\nâŒ Error: {e}")

if __name__ == "__main__":
    main()
